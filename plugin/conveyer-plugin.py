#!/usr/bin/env python
"""
conveyer-plugin.py rotates a conveyer log and processes the data, handing it
off to Rackspace Cloud Monitoring.

To invoke this program, you must have CONVEYER_AGENT_PLUGIN_CONFIG environment
variable set to the location of a JSON file.  You can find an example config
file in the same directory as this program.  If you do not have
CONVEYER_AGENT_PLUGIN_CONFIG set, the program will default to the filename
/etc/conveyer-plugin.json.  If that file does not exist, this program will
raise an exception.
"""

import json
import requests
import os


config = {
    "state_file_name": None,
    "conveyer_url": None,
    "metrics": [],
}


accumulators = dict()
deltas = {
    "conveyer.events.bad_json": 0,
    "conveyer.logfile.missing": 0,
}
state = None


def recover_config(where):
    """
    Read configuration from persistent storage.

    :param str where: The location of the configuration file.
    """
    global config
    config = json.load(file(where))


def recover_state():
    """
    Read the most recent measurements from a state file.  The filename for
    the state file will be established in the configuration file's
    "state_file_name" key.
    """
    global state
    try:
        state = json.load(file(config["state_file_name"]))
    except:
        state = dict()


def initialize_accumulators():
    """
    Start our accumulators off at the last known values.  If a new accumulator
    is defined since we last ran, we start it off at zero.
    """
    # candidate to refactor with nearly common code in accumulate_event.
    global accumulators
    for key in config["metrics"]:
        accumulators[key] = state.get(key, 0)


def accumulate_event(event):
    """
    Take the latest reading as our definitive measurement for a given stat.
    Since Logstash is not configured to automatically zero measurements, this is
    guaranteed to be the largest known measurement for this log rotation.

    :param str event: A JSON-encoded event string; for details, consult the
        documentation for Logstash.
    """
    global accumulators
    for key in config["metrics"]:
        accumulators[key] = event.get(key, 0)


def update_state():
    """
    Updates in-memory understanding of the current state of metrics with the
    latest values.  As well, it calculates deltas from the previous run of the
    program.
    """
    global state, deltas
    for key in accumulators:
        new_value = accumulators[key]
        old_value = state.get(key, 0)
        state[key] = new_value
        deltas[key] = max(new_value - old_value, 0)


def accumulate(events):
    """
    Loop through each received event, taken from the list given, and accumulate
    the information contained within.

    :param list events: A list of JSON-encoded events, as would be generated by
        Logstash, for instance.
    """
    for event in events:
        try:
            e = json.loads(event)
            accumulate_event(e)
        except:
            deltas["conveyer.events.bad_json"] += 1


def persist_state():
    """Write our current state back to persistent store for next time."""
    file(config["state_file_name"], "w").write(json.dumps(state))


def report():
    """
    Generates a report of processed metrics, in a format which Cloud Monitoring
    Agent can understand.
    """
    print "status ok metrics follow."
    for key in deltas:
        print "metric {0} float {1}".format(key, deltas[key])


def main():
    """Main entry point for the program."""
    config_location = os.environ.get(
        "CONVEYER_AGENT_PLUGIN_CONFIG",
        "/etc/conveyer_plugin.json"
    )
    recover_config(config_location)
    recover_state()
    r = requests.post(config["conveyer_url"])
    log_file_name = r.text
    initialize_accumulators()
    if r.status_code == 200:
        contents = file(log_file_name).readlines()
        accumulate(contents)
    else:
        deltas["conveyer.logfile.missing"] += 1
    update_state()
    persist_state()
    report()


if __name__ == "__main__":
    main()
